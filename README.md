# Machine-Learning-Workshop
### Week 1: 機器學習基石Overview `泰瑋`

### Week 2: Linear Regression `宛誼`
- 機器學習基石
> 1. https://www.youtube.com/watch?v=qGzjYrLV-4Y&index=34&list=PLXVfgk9fNX2I7tB6oIINGBmW50rrmFTqf
> 2. https://www.youtube.com/watch?v=2LfdSCdcg1g&index=35&list=PLXVfgk9fNX2I7tB6oIINGBmW50rrmFTqf
> 3. https://www.youtube.com/watch?v=lj2jK1FSwgo&index=36&list=PLXVfgk9fNX2I7tB6oIINGBmW50rrmFTqf
> 4. https://www.youtube.com/watch?v=tF1HTirYbtc&index=37&list=PLXVfgk9fNX2I7tB6oIINGBmW50rrmFTqf

- Scikit-learn
> http://scikit-learn.org/stable/modules/linear_model.html#linear-model
> 1. Ordinary Least Squares
> 2. Ridge Regression
> 3. Lasso

### Week 3: Logistic Regression `信賢`

### Week 4: Classification And Regression Tree (CART)

### Week 5: Random Forest

### Week 6: Ensemble
- Two families of ensemble methods are usually distinguished:
> 1. In averaging methods, the driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced.
> Examples: Bagging methods, Forests of randomized trees

> 2. By contrast, in boosting methods, base estimators are built sequentially and one tries to reduce the bias of the combined estimator. The motivation is to combine several weak models to produce a powerful ensemble.
> Examples: AdaBoost, Gradient Tree Boosting

### Week 7: Gradient Boosting Machine (GBM)

### Week 8: eXtreme Gradient Boosting (XGBoost)

### Week 9: Neural Networks
